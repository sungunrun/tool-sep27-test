<div id="PGHEADER">
514 Perceptual-Level Conation and Action</div>
<br><br>
Perceptual-Level Attention Commands and Guidance of Saccades. <br>
Active eye movements are probably more often independent of background propositional <br>
attitudes than reaching. In many human reaches, but probably not all, propositional level <br>
intentions cause a sub-propositional actional targeting state. This state borrows some or <br>
all of its content from perception. I follow motor psychology in being neutral in any given <br>
case on whether there is a propositional-level background cause. Proximate psychological <br>
causes of active motor behavior are sub-propositional conative command states and per-<br>
ceptual states. Computational models that explain perceptually guided action start <br>
with them.<br>
<br>
In the last three decades, it has been established that, in most mammals, the dominant <br>
coordinate system for guiding reaching and other primitive bodily actions is visual. A tar-<br>
get is usually set visually. The dominant spatial coordinates for reaching are set by this <br>
initial perception. The dominant coordinates could have been proprioceptive—for <br>
ex ample, coordinates whose anchor point is at the shoulder or hand. But they tend not to <br>
be.690 Of course, one can continue a reaching operation initially fixed visually, without <br>
looking. In such cases, the initial position of the target is remembered, and motor com-<br>
mands are translated and oriented accordingly.<br>
<br>
Although one can sometimes see and visually monitor one’s hand movement, reaching <br>
is usually carried out as eyes fix the target-object, not the hand. The main source of infor-<br>
mation about hand position is proprioceptive. Visual information functions as a supple-<br>
ment.691 At the outset of a reach, the actional system needs information about the hand’s <br>
location. This is normally provided by proprioception. The proprioceptive system has one <br>
or more spatial coordinate systems of its own, with different anchor points from that (or <br>
those) of the visual system.692<br>
<br>
The kinematic problem of reaching consists, schematically, of three main stages.693<br>
<br>
First, the agent’s psychological system must locate initial positions of the hand and the <br>
target-object. Localization of target-object is usually by vision. Localization of the hand is <br>
by proprioception. Ignoring orientation of the hand with respect to the wrist and orienta-<br>
tion of the fingers and their parts, the location of the hand can be specified in a shoulder-<br>
anchored proprioceptive coordinate system by specifying five angles of parts of the arm in <br>
a three-dimensional Cartesian coordinate system. The upper arm has three degrees of <br>
freedom with respect to the shoulder. One can pronate (rotate toward the center) or su pin-<br>
ate (rotate toward the side) the upper arm. One can abduct (raise the arm outwardly to the <br>
side) or adduct (bring the arm inward toward or across one’s body). And one can flex <br>
(move the arm forward) or extend (move it backward). The lower arm has two degrees of <br>
freedom with respect to the elbow. One can pronate or supinate it; and one can flex it or <br>
extend it. Specification of the three angles that define the angle of the upper arm with <br><br>

<div id= 'FOOTNOTES'>
<b>FOOTNOTES</b><br>
<b>FN</b>690 C. Buneo, R. Jarvis, A. Batista, and R. Andersen, ‘Direct Visuo-Motor Transformations for Reaching’, <br>
<i>Nature</i> 416 (2002), 632–636; S. Deneve, P. Latham, and A. Pouget, ‘Efficient Computation and Cue Integration <br>
with Noisy Population Codes’, <i>Nature Neuroscience</i> 4 (2001), 826–831.<br>
<b>FN</b>691 M. Heath, ‘Role of Limb and Target Vision in the Online Control of Memory-Guided Reaches’, <i>Motor <br>
Control</i> 9 (2005), 281–311.<br>
<b>FN</b>692 In fact, multiple coordinate systems figure in relatively primitive action and in motor learning. <br>
See M. Berniker, D. Franklin, J. Flanagan, D. Wolpert, and K. Kording, ‘Motor Learning of Novel Dynamics is not <br>
Represented in a Single Global Coordinate System: Evaluation of Mixed Coordinate Representations and Local <br>
Learning’, <i>Journal of Neurophysiology</i> 111 (2014), 1165–1182.<br>
<b>FN</b>693 A good overview of these stages can be found in R. Shadmehr and S. Wise, <i>The Computational Neurobiology <br>
of Reaching and Pointing</i> (Cambridge, MA: MIT Press, 2005), chapters 9–19.</div>

