<div id="PGHEADER">
Executive Control System, Propositional Drivers 561</div>
<br><br>
15.3a, 15.3b, and 15.3c for detailed flow charts of the model. These should be consulted in <br>
following the discussion of the model in the just-preceding paragraph and in subsequent <br>
paragraphs.<br>
<br>
The program postulates two types of operations. One is called ‘Methods’, which are com-<br>
prised of various types of operation, each adapted to task or input. For example, a Method <br>
that encodes how to perform visual search involves a specification of the target being sought. <br>
It might be tuned to light levels and contextual perceptual information, useful to shaping <br>
attentive behavior. Methods are rules of operation, not states with representational content. <br>
They involve conative elements. But there is nothing in the account that places them at a <br>
higher level of operation than that of perception and relatively primitive conation (see <br>
Chapter 14). The authors’ flow chart looks very similar to the conative flow chart, Figure 14.1, <br>
for initiation of motor behavior (Chapter 14, the section Broader Structure of <br>
Conation in Causing Relatively Primitive Action). The second type of oper-<br>
ation is called ‘Scripts’. These are more specific ‘executable versions of tuned methods’. The <br>
visual task executive ‘reads [discriminates] tasks [task specifications], selects task methods, <br>
tunes methods into executable scripts, and monitors and adapts script progress’. Both <br>
Methods and Scripts operations have access to memory, yes–no decision points decided by <br>
the execution of particular functions, and control of signal settings. The operations com-<br>
pound ‘more primitive elements’ to yield such actions as visual boundary tracing.<br>
<br>
Initiations of attention involve inhibiting former attended pathways, lifting surround <br>
suppression, selecting new foci of attention (area, property, object), sending a signal to <br>
select a new focus, and initiating new surround suppression. Initiations are based on mem-<br>
ories of routines in fulfilling tasks and on discrimination of objects, object parts, properties, <br>
and object categories. Memory is organized largely associatively to match Methods with <br>
task specifications. Initiations of attention operate on various stages of the visual hierarchy, <br>
but only on the visual hierarchy. Selection of foci of attention depends on two maps. One is <br>
a peripheral field priority map, which represents saliency in the peripheral visual field, <br>
biased by task. The other is a history-biased priority map that locates the focus of attention <br>
derived from the central visual field after processing in the visual hier archy. New attention <br>
initiations, overt and covert, start usually from central to peripheral areas of the visual field, <br>
but may be within central areas. They derive from combining visual information from the <br>
two maps. Short-term visual memory stores the most recent attention location(s). (Geisler’s <br>
model—see this chapter, previous section—requires retention of only the most recent loca-<br>
tion.) Feedback loops occur from efference copy, monitoring success in carrying out the <br>
specified task. None of these operations is supra-perceptual.<br>
<br>
That is a sketch of the operations. Since the representations derive from the visual hier-<br>
archy, the representational content—including categorization—utilized in the computa-<br>
tion is taken to be from perception, either present perception or stored perceptual <br>
representations in perceptual memory. Categorization, recognition, identification—pri-<br>
mary uses of representational contents—are understood in the model as uses of perceptual <br>
representations.<br>
<br>
Categorization is ‘classification that connects each stimulus to a class of similar stim-<br>
uli’, where classification is explicitly taken to occur within the standard visual hierarchy.798 <br><br>

<div id= 'FOOTNOTES'>
<b>FOOTNOTES</b><br>
<b>FN</b>798 This characterization and the following ones are taken from J. Tsotsos, <i>A Computational Perspective on <br>
Visual Attention</i> (Cambridge, MA: The MIT Press, 2011), 134–139.AQ:3</div>

